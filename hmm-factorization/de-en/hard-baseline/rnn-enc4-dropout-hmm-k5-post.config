#!crnn/rnn.py


import os
from subprocess import check_output
import numpy

my_dir = os.path.dirname(os.path.abspath(__file__))

debug_mode = False
if int(os.environ.get("DEBUG", "0")):
    print("** DEBUG MODE")
    debug_mode = True

if config.has("beam_size"):
    beam_size = config.int("beam_size", 0)
    print("** beam_size %i" % beam_size)
else:
    beam_size = 12

# task
use_tensorflow = True
task = "train"
device = "gpu"
multiprocessing = True
update_on_device = True

# data
num_inputs = 45412
num_outputs = {'data': [45412, 1], 'classes': [33525, 1]}
num_seqs = {'train': 5920281}
EpochSplit = 200
SeqOrderTrainBins = num_seqs["train"] // 1000
TrainSeqOrder = "laplace:%i" % SeqOrderTrainBins
if debug_mode:
    TrainSeqOrder = "default"

_cf_cache = {}

def cf(filename):
    """Cache manager"""
    if filename in _cf_cache:
        return _cf_cache[filename]
    if int(os.environ.get("CF_NOT_FOR_LOCAL", "1")) and check_output(["hostname"]).strip() in ["cluster-cn-211", "sulfid"]:
        print("use local file: %s" % filename)
        return filename  # for debugging
    cached_fn = check_output(["cf", filename]).strip()
    assert os.path.exists(cached_fn)
    _cf_cache[filename] = cached_fn
    return cached_fn

def get_dataset(data):
    epochSplit = {"train": EpochSplit}.get(data, 1)
    return {
        "class": "TranslationDataset",
        "path": "/u/bahar/workspace/wmt/2018/de-en-6M--2019-01-16/de-en-hmm--2018-01-16/dataset",
        "file_postfix": data,
        "source_postfix": " </S>",
        "target_postfix": " </S>",
        'unknown_label' : "<UNK>",
        "partition_epoch": epochSplit,
        "seq_ordering": {"train": TrainSeqOrder, "dev": "sorted"}.get(data, "default"),
        "estimated_num_seqs": (num_seqs.get(data, None) // epochSplit) if data in num_seqs else None}

train = get_dataset("train")
dev = get_dataset("dev")
newstest2015 = get_dataset("newstest2015")
newstest2016 = get_dataset("newstest2016")
newstest2017 = get_dataset("newstest2017")
newstest2018 = get_dataset("newstest2018")
cache_size = "0"
window = 1
# HMM Factorization data
vocab_size = num_outputs["classes"][0]
intermediary_size = 1000

# network
network = {
"source_embed": {"class": "linear", "activation": None, "with_bias": False, "n_out": 512},

"lstm0_fw" : { "class": "rec", "unit": "nativelstm2", "n_out" : 1000, "direction": 1, "from": ["source_embed"] , "dropout": 0.3},
"lstm0_bw" : { "class": "rec", "unit": "nativelstm2", "n_out" : 1000, "direction": -1, "from": ["source_embed"], "dropout": 0.3},

"lstm1_fw" : { "class": "rec", "unit": "nativelstm2", "n_out" : 1000, "direction": 1, "from": ["lstm0_fw", "lstm0_bw"] , "dropout": 0.3},
"lstm1_bw" : { "class": "rec", "unit": "nativelstm2", "n_out" : 1000, "direction": -1, "from": ["lstm0_fw", "lstm0_bw"], "dropout": 0.3},

"lstm2_fw" : { "class": "rec", "unit": "nativelstm2", "n_out" : 1000, "direction": 1, "from": ["lstm1_fw", "lstm1_bw"] , "dropout": 0.3},
"lstm2_bw" : { "class": "rec", "unit": "nativelstm2", "n_out" : 1000, "direction": -1, "from": ["lstm1_fw", "lstm1_bw"] , "dropout": 0.3},

"lstm3_fw" : { "class": "rec", "unit": "nativelstm2", "n_out" : 1000, "direction": 1, "from": ["lstm2_fw", "lstm2_bw"] , "dropout": 0.3},
"lstm3_bw" : { "class": "rec", "unit": "nativelstm2", "n_out" : 1000, "direction": -1, "from": ["lstm2_fw", "lstm2_bw"] , "dropout": 0.3},

"encoder": {"class": "copy", "from": ["lstm3_fw", "lstm3_bw"]},
"enc_ctx": {"class": "linear", "activation": None, "with_bias": True, "from": ["encoder"], "n_out": 1000},  # preprocessed_attended in Blocks
"inv_fertility": {"class": "linear", "activation": "sigmoid", "with_bias": False, "from": ["encoder"], "n_out": 1},

"output": {"class": "rec", "from": [], "unit": {
    'output': {'class': 'choice', 'target': 'classes', 'beam_size': beam_size, 'from': ["output_prob"], "initial_output": 0},
    "end": {"class": "compare", "from": ["output"], "value": 0},
    'target_embed': {'class': 'linear', 'activation': None, "with_bias": False, 'from': ['output'], "n_out": 512, "initial_output": 0},  
    "weight_feedback": {"class": "linear", "activation": None, "with_bias": False, "from": ["prev:accum_att_weights"], "n_out": 1000},
    "prev_s_state": {"class": "get_last_hidden_state", "from": ["prev:s"], "n_out": 2000},
    "prev_s_transformed": {"class": "linear", "activation": None, "with_bias": False, "from": ["prev_s_state"], "n_out": 1000},
    "energy_in": {"class": "combine", "kind": "add", "from": ["base:enc_ctx", "weight_feedback", "prev_s_transformed"], "n_out": 1000},
    "energy_tanh": {"class": "activation", "activation": "tanh", "from": ["energy_in"]},
    "energy": {"class": "linear", "activation": None, "with_bias": False, "from": ["energy_tanh"], "n_out": 1},  
    "att_weights": {"class": "softmax_over_spatial", "from": ["energy"]}, 
    "accum_att_weights": {"class": "eval", "from": ["prev:accum_att_weights", "att_weights", "base:inv_fertility"],
        "eval": "source(0) + source(1) * source(2) * 0.5", "out_type": {"dim": 1, "shape": (None, 1)}},
    "att": {"class": "generic_attention", "weights": "att_weights", "base": "base:encoder"},
    "s": {"class": "rnn_cell", "unit": "LSTMBlock", "from": ["target_embed", "att"], "n_out": 1000}, 
    # Lexicon model
    # Prev output (B, embed_dim) --> (B, intermediary_size)
    "prev_outputs_transformed": {"class": "linear", "activation": None, "with_bias": False, "from": ["prev:target_embed"], "n_out": intermediary_size},

    # Prev state (B, hidden) --> (B, intermediary_size)
    "prev_state_transformed": {"class": "linear", "activation": None, "with_bias": False, "from": ["prev_s_state"], "n_out": intermediary_size},
    "base_encoder": {"class": "linear", "activation": None, "with_bias": False, "from": ["base:enc_ctx"], "n_out": intermediary_size},
    
    "prev_prev_state_transformed": {"class": "copy", "from": ["prev:prev_state_transformed"]},     
    
    "output_prob" : {"class": "hmm_factorization",
                     "attention_weights": "att_weights",
                     "base_encoder_transformed": "base_encoder",
                     "prev_state": "prev_state_transformed",
                     "prev_outputs": "prev_outputs_transformed",
                     "debug": False,
                     "first_order_alignments": True,
                     "prev_prev_state": "prev_prev_state_transformed",
		             "top_k": 5,
                     "target": "classes", "loss": "ce",
		             "n_out": num_outputs['classes'][0]},

}, "target": "classes", "max_seq_len": "max_len_from('base:encoder') * 3", "optimize_move_layers_out": True},

"decision": {
    "class": "decide", "from": ["output"], "loss": "edit_distance", "target": "classes",
    "loss_opts": {
        "debug_print": False
        }
    }
}
search_output_layer = "decision"
debug_print_layer_output_template = True

# trainer
batching = "random"
accum_grad_multiple_step = 4 #1
batch_size = 800
max_seqs = 80
max_seq_length = 75
truncation = -1
num_epochs = 500
model = "net-model/network"
cleanup_old_models = True

def custom_construction_algo(idx, net_dict):
    # For debugging, use: python3 ./crnn/Pretrain.py config...
    orig_num_lstm_layers = 0
    while "lstm%i_fw" % orig_num_lstm_layers in net_dict:
        orig_num_lstm_layers += 1
    num_lstm_layers = idx + 1  # idx starts at 0. start with 1 layer
    # Use label smoothing only at the very end.
    # net_dict["output"]["unit"]["output_prob"]["loss_opts"]["label_smoothing"] = 0
    if num_lstm_layers == orig_num_lstm_layers:
        return net_dict
    if num_lstm_layers >= orig_num_lstm_layers:
        return None
    # Leave the last layer as-is, but only modify its source.
    net_dict["encoder"]["from"] = ["lstm%i_fw" % (num_lstm_layers - 1), "lstm%i_bw" % (num_lstm_layers - 1)]
    # Delete non-used lstm layers. This is not explicitly necessary but maybe nicer.
    for i in range(num_lstm_layers, orig_num_lstm_layers):
        del net_dict["lstm%i_fw" % i]
        del net_dict["lstm%i_bw" % i]
    return net_dict

#pretrain = {"repetitions": 5, "construction_algo": custom_construction_algo}
gradient_clip = 0
adam = True
optimizer_epsilon = 1e-8
debug_add_check_numerics_on_output = True
tf_log_memory_usage = True
gradient_noise = 0.0
learning_rate = 0.0005
learning_rate_control = "newbob_multi_epoch"
learning_rate_control_relative_error_relative_lr = True
learning_rate_control_min_num_epochs_per_new_lr = 10
newbob_multi_num_epochs = 10
newbob_multi_update_interval = 1
newbob_learning_rate_decay = 0.9
learning_rate_file = "newbob.data"

# log
log_verbosity = 5



# Config-file copied for logging purpose by Returnn.
# Time: 2019-05-23 14:16:13
# Call: ['/work/smt2/makarov/returnn-hmm/tools/get-attention-weights.py', '/u/bahar/workspace/wmt/2018/de-en-6M--2019-01-16/de-en-hmm--2018-01-16/config-train/rnn-enc4-dropout3-hmm-k5.config', '--epoch', '138', '--data', 'config:newstest2018', '--dump_dir', '/work/smt2/makarov/NMT/hmm-factorization/experiments/wmt2019/parnia-de-en-rnn-k5', '--layers', 'att_weights', '--rec_layer', 'output', '--batch_size', '600', '--tf_log_dir', '/work/smt2/makarov/NMT/hmm-factorization/experiments/wmt2019/parnia-de-en-rnn-k5/tf_log_dir/', '--hmm_fac_fo']
# Path: /work/smt3/bahar/expriments/wmt/2018/de-en/de-en-6M--2019-01-16/de-en-hmm--2018-01-16/data-train/rnn-enc4-dropout3-hmm-k5
# Hostname: cluster-cn-223
# PID: 20610
# Returnn: 20190521.093826--git-1e2123a-dirty
# TensorFlow: 1.10.0 (v1.10.0-0-g656e7a2b34) (<site-package> in /u/bahar/settings/python3-returnn-tf1.9/lib/python3.5/site-packages/tensorflow)
# Config files: ['/u/bahar/workspace/wmt/2018/de-en-6M--2019-01-16/de-en-hmm--2018-01-16/config-train/rnn-enc4-dropout3-hmm-k5.config']

