# How to visualize attention weights

# General Info
The process is split into 2 steps, the first is getting the attention weights from the model (needs to be done 1 time), and then visualizing (as often as needed).

The first step is a bit tricky, and is dependent on the returnn version. There is a script to get attention weights in each returnn version, but I've got a modifed version which supports search, multihead attention and self-attention (though the last one very hackily). It is located here: https://github.com/nikita68/returnn/blob/master/tools/get-attention-weights.py
After running this script (usually about 40 mins), you get lots of numpy arrays for the attention weights which can be visualized using the visualization script.

# Step 1: Launching 

The launching process can be done using a script, which can be found here for newstest<YEAR> transformer cases: https://github.com/nikita68/NMT/blob/master/attention_weights.sh
This script can be launched as a job on the cluster using the following parameters: ```... attention_weights.sh <year> <path_to_config> <epoch> <beam_size> <output_folder> <model_directory>```. Due to how returnn is built, this script <b>HAS</b> to be launched from the directory where ```net-model``` is located. For example, for the transformer, it needs to be launched from: ```/work/smt2/makarov/NMT/hmm-factorization/de-en/hard-baseline/logs/transformer-newBaseline```.

This bash script then calls the python script to get the attention weights. 

Another parameter which might need to be set is ```--tf_log_dir <path>```, which needs to be set to some place to dump the tensorflow log files, but only if you do not have write access to the model directory folder!

By setting ``--do_search`` as a parameter, the attention weights will be retrieved for search.

<b>WARNING:</b> different versions of RETURNN use different ways of saving the attention weights. This means that if you have a newer version you might need to set the dimension order differently in ```get-attention-weights.py```, usually around lines 312 and 343.

# Step 2: Actual Visualization
In general, the script is located in ```python3 /work/smt2/makarov/NMT/visualizations/attention_weights/visualization_attention_returnn.py <filename> <time index>```.

All parameters are documented, but the most important parts is the filename and the time index. The filename should be generated by ```get-attention-weights.py``` file in returnn.

The parameter ```--layer_to_viz <layer_name>``` allows you to select a single layer to visualize, whilst ```--all_layers``` will show all layers.
If you also use ```--multihead``` it will show all heads, either for all layers or for the currently selected layer. If this option is not activated, then the script with average over heads.
You can also save figures in high resolution using ```--save_fig <path>```.

### ASR

When using ASR attention weights, you need to activate the ```--asr``` parameter, as well as set the correct target dictionary, which is done with ``--target_vocab_file``.

In most cases, just set to ```--target_vocab_file /u/bahar/workspace/asr/librispeech/test-20190121/dataset/trans.bpe.vocab```.

In the examples below the file and time index are already set, but you need to change them to see other examples! 

#### Transformer

Navigate to ```/work/smt2/makarov/NMT/hmm-factorization/experiments/asr2019/transformer-data-augmentation/forward-dev-other```. There are also ```forward-dev-clean, forward-test-other and forward-test-clean```.

Then run ```python3 /work/smt2/makarov/NMT/visualizations/attention_weights/visualization_attention_returnn.py trafo.specaug.datarndperm_noscale.12l.ffdim4.pretrain3.hpc_ep481_data_1177_1181.npy 0 --asr --target_vocab_file /u/bahar/workspace/asr/librispeech/test-20190121/dataset/trans.bpe.vocab --all_layers```, to visualize average over all layers.
All parameters can work as in the general case, with attention layers being called ```rec_dec_01_att_weights``` to ```rec_dec_12_att_weights```.

For example to visualize layer nr. 4 with all attention heads:
```python3 /work/smt2/makarov/NMT/visualizations/attention_weights/visualization_attention_returnn.py trafo.specaug.datarndperm_noscale.12l.ffdim4.pretrain3.hpc_ep481_data_1177_1181.npy 0 --asr --target_vocab_file /u/bahar/workspace/asr/librispeech/test-20190121/dataset/trans.bpe.vocab --layer_to_viz rec_dec_04_att_weights --multihead```

#### Transformer Prior

Navigate to ```/work/smt2/makarov/NMT/hmm-factorization/experiments/asr2019/transformer-prior-data-augmentation/forward-dev-other```. There are also ```forward-dev-clean, forward-test-other and forward-test-clean```.

Then run ```python3 /work/smt2/makarov/NMT/visualizations/attention_weights/visualization_attention_returnn.py trafo.specaug.datarndperm_noscale.12l.ffdim4.pretrain3-prior-k6_ep560_data_1111_1114.npy 0 --asr --target_vocab_file /u/bahar/workspace/asr/librispeech/test-20190121/dataset/trans.bpe.vocab --all_layers```, to visualize average over all layers.

#### LSTM
Navigate to ```/work/smt2/makarov/NMT/hmm-factorization/experiments/asr2019/lstm/forward-dev-other```.

Then run ```python3 /work/smt2/makarov/NMT/visualizations/attention_weights/visualization_attention_returnn.py base2.smlp2.specaug.datarndperm_noscale.bs18k.curric3.retrain1_ep250_data_1202_1206.npy 0 --asr --target_vocab_file /u/bahar/workspace/asr/librispeech/test-20190121/dataset/trans.bpe.vocab --all_layers```

#### LSTM Prior
NOTE: still not working

