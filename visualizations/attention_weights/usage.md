# How to visualize attention weights

# General
In general, the script is located in ```python3 /work/smt2/makarov/NMT/visualizations/attention_weights/visualization_attention_returnn.py <filename> <time index>```.

All parameters are documented, but the most important parts is the filename and the time index. The filename should be generated by ```get-attention-weights.py``` file in returnn.

The parameter ```--layer_to_viz <layer_name>``` allows you to select a single layer to visualize, whilst ```--all_layers``` will show all layers.
If you also use ```--multihead``` it will show all heads, either for all layers or for the currently selected layer. If this option is not activated, then the script with average over heads.
You can also save figures in high resolution using ```--save_fig <path>```.

# ASR

When using ASR attention weights, you need to activate the ```--asr``` parameter, as well as set the correct target dictionary, which is done with ``--target_vocab_file``.

In most cases, just set to ```--target_vocab_file /u/bahar/workspace/asr/librispeech/test-20190121/dataset/trans.bpe.vocab```.


## Transformer

Navigate to ```/work/smt2/makarov/NMT/hmm-factorization/experiments/asr2019/transformer-data-augmentation/forward-dev-other```. There are also ```forward-dev-clean, forward-test-other and forward-test-clean```.

Then run ```python3 /work/smt2/makarov/NMT/visualizations/attention_weights/visualization_attention_returnn.py trafo.specaug.datarndperm_noscale.12l.ffdim4.pretrain3.hpc_ep481_data_1177_1181.npy 0 --asr --target_vocab_file /u/bahar/workspace/asr/librispeech/test-20190121/dataset/trans.bpe.vocab --all_layers```, to visualize average over all layers.
All parameters can work as in the general case, with attention layers being called ```rec_dec_01_att_weights``` to ```rec_dec_12_att_weights```.

For example to visualize layer nr. 4 with all attention heads:
```python3 /work/smt2/makarov/NMT/visualizations/attention_weights/visualization_attention_returnn.py trafo.specaug.datarndperm_noscale.12l.ffdim4.pretrain3.hpc_ep481_data_1177_1181.npy 0 --asr --target_vocab_file /u/bahar/workspace/asr/librispeech/test-20190121/dataset/trans.bpe.vocab --layer_to_viz rec_dec_04_att_weights --multihead```

## Transformer Prior

Navigate to ```/work/smt2/makarov/NMT/hmm-factorization/experiments/asr2019/transformer-prior-data-augmentation/forward-dev-other```. There are also ```forward-dev-clean, forward-test-other and forward-test-clean```.

Then run ```python3 /work/smt2/makarov/NMT/visualizations/attention_weights/visualization_attention_returnn.py trafo.specaug.datarndperm_noscale.12l.ffdim4.pretrain3-prior-k6_ep560_data_1111_1114.npy 0 --asr --target_vocab_file /u/bahar/workspace/asr/librispeech/test-20190121/dataset/trans.bpe.vocab --all_layers```, to visualize average over all layers.

## LSTM
Navigate to ```/work/smt2/makarov/NMT/hmm-factorization/experiments/asr2019/lstm/forward-dev-other```.

Then run ```python3 /work/smt2/makarov/NMT/visualizations/attention_weights/visualization_attention_returnn.py base2.smlp2.specaug.datarndperm_noscale.bs18k.curric3.retrain1_ep250_data_1202_1206.npy 0 --asr --target_vocab_file /u/bahar/workspace/asr/librispeech/test-20190121/dataset/trans.bpe.vocab --all_layers```

## LSTM Prior
NOTE: still not working

