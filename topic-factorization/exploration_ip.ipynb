{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Topic and Word Relationships\n",
    "\n",
    "Here we explore different ways that the the newstest 2015 dataset can be looked at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file\n",
      "Finished loading in file\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def read_file(file_path, max_amount=math.inf):\n",
    "    \"\"\"\n",
    "    Reads a number of lines from file and return list of lines\n",
    "    :param file_path:\n",
    "    :param max_amount:\n",
    "    :return: list[str]\n",
    "    \"\"\"\n",
    "    line_list = []\n",
    "\n",
    "    with open(file=file_path) as f:\n",
    "        curr_idx = 0\n",
    "        while curr_idx < max_amount:\n",
    "            line_list.append(f.readline())\n",
    "            curr_idx += 1\n",
    "\n",
    "    return line_list\n",
    "\n",
    "print(\"Loading file\")\n",
    "l_raw = read_file('./trg.shuf', 5000000)\n",
    "print(\"Finished loading in file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/nikita/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/nikita/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size small: 17870\n",
      "Vocab size full: 1001244\n",
      "Finished Dictionary\n",
      "Finished corpus\n",
      "Topics: \n",
      "(0, '0.000*\"august\" + 0.000*\"tuesday\" + 0.000*\"2016\" + 0.000*\"november\"')\n",
      "(1, '0.001*\"interest\" + 0.000*\"visit\" + 0.000*\"town\" + 0.000*\"2014\"')\n",
      "(2, '0.003*\"say\" + 0.003*\"one\" + 0.002*\"also\" + 0.002*\"time\"')\n",
      "(3, '0.001*\"2012\" + 0.001*\"page\" + 0.001*\"law\" + 0.001*\"demand\"')\n",
      "(4, '0.001*\"2016\" + 0.001*\"post\" + 0.001*\"http\" + 0.001*\"currently\"')\n",
      "(5, '0.002*\"night\" + 0.001*\"sat\" + 0.000*\"method\" + 0.000*\"maintain\"')\n",
      "(6, '0.001*\"two\" + 0.001*\"report\" + 0.001*\"city\" + 0.001*\"find\"')\n",
      "(7, '0.001*\"2015\" + 0.000*\"sunday\" + 0.000*\"march\" + 0.000*\"thursday\"')\n",
      "(8, '0.005*\"amp\" + 0.000*\"delivery\" + 0.000*\"south\" + 0.000*\"http\"')\n",
      "(9, '0.001*\"try\" + 0.000*\"practice\" + 0.000*\"spend\" + 0.000*\"safety\"')\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import operator\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "\n",
    "\n",
    "def process_word(word, remove_stop=False, lemmatize_and_morph=True):\n",
    "    \"\"\"\n",
    "    Returns process word\n",
    "    :param word:\n",
    "    :return: str word\n",
    "    \"\"\"\n",
    "    # lower\n",
    "    word = word.lower()\n",
    "    \n",
    "    # Remove symbols\n",
    "    if len(word) < 3:\n",
    "        return None\n",
    "    \n",
    "    # Get morph\n",
    "    if lemmatize_and_morph is True:\n",
    "        t_word = wn.morphy(word)\n",
    "        if t_word is not None:\n",
    "            word = t_word\n",
    "\n",
    "        # Get lemma\n",
    "        word = WordNetLemmatizer().lemmatize(word)\n",
    "    \n",
    "    # Remove stop\n",
    "    if remove_stop is True and word in en_stop:\n",
    "        return None\n",
    "    return word\n",
    "\n",
    "\n",
    "def preprocess_all_words(word_list):\n",
    "    ret = []\n",
    "    for line in word_list:\n",
    "        temp = []\n",
    "        for word in line:\n",
    "            w = process_word(word, remove_stop=True, lemmatize_and_morph=False)\n",
    "            if w is not None:\n",
    "                temp.append(w)\n",
    "        ret.append(temp)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def process_bpe(line_list, seq=\"@@\", rm_words=[\"\\n\"], lemmatize_and_morph=True):\n",
    "    \"\"\"\n",
    "    Transforms line list form read_file to list[list[str=words]], and bpe words merged together\n",
    "    :param line_list:\n",
    "    :param seq:\n",
    "    :param rm_words:\n",
    "    :return: list[list[str=words]]\n",
    "    \"\"\"\n",
    "    full_word_list = []\n",
    "    for line in line_list:\n",
    "        temp_words = line.split()\n",
    "        full_words = []\n",
    "        idx = 0\n",
    "        while idx < len(temp_words):\n",
    "            if temp_words[idx].endswith(seq):\n",
    "                temp_str = \"\"\n",
    "                while temp_words[idx].endswith(seq):\n",
    "                    temp_str += temp_words[idx][:-(len(seq))]\n",
    "                    idx += 1\n",
    "                temp_str += temp_words[idx]\n",
    "                if temp_str not in rm_words:\n",
    "                    w = process_word(temp_str, lemmatize_and_morph=lemmatize_and_morph)\n",
    "                    if w is not None:\n",
    "                        full_words.append(w)\n",
    "                idx += 1\n",
    "            else:\n",
    "                if temp_words[idx] not in rm_words:\n",
    "                    w = process_word(temp_words[idx], lemmatize_and_morph=lemmatize_and_morph)\n",
    "                    if w is not None:\n",
    "                        full_words.append(w)\n",
    "                idx += 1\n",
    "        full_word_list.append(full_words)\n",
    "    return full_word_list\n",
    "\n",
    "\n",
    "def get_word_count(word_list):\n",
    "    \"\"\"\n",
    "    Returns word count.\n",
    "    :param word_list:\n",
    "    :return: List[(str=word, int=count)]\n",
    "    \"\"\"\n",
    "    word_count = {}\n",
    "    for line in word_list:\n",
    "        for word, val in Counter(line).most_common():\n",
    "            if word in word_count:\n",
    "                word_count[word] += val\n",
    "            else:\n",
    "                word_count[word] = val\n",
    "\n",
    "    return sorted(word_count.items(), key=operator.itemgetter(1))\n",
    "\n",
    "\n",
    "def get_topics_lda(word_list, dictionary, num_topics=10):\n",
    "    corpus = [dictionary.doc2bow(text) for text in word_list]\n",
    "    print(\"Finished corpus\")\n",
    "    \n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)\n",
    "    topics = ldamodel.print_topics(num_words=4)\n",
    "    print(\"Topics: \")\n",
    "    for topic in topics:\n",
    "        print(topic)\n",
    "\n",
    "    return topics, ldamodel, corpus, dictionary\n",
    "\n",
    "import random\n",
    "l = random.sample(l_raw, 10000)\n",
    "\n",
    "wl = process_bpe(line_list=l)\n",
    "print(\"Vocab size train: \" + str(len(get_word_count(word_list=wl))))\n",
    "\n",
    "wl_full = process_bpe(line_list=l_raw, lemmatize_and_morph=False)\n",
    "print(\"Vocab size full: \" + str(len(get_word_count(word_list=wl_full))))\n",
    "\n",
    "dictionary = corpora.Dictionary(wl_full)\n",
    "print(\"Finished Dictionary\")\n",
    "\n",
    "t, ldamodel,_, dictionary = get_topics_lda(word_list=preprocess_all_words(wl), dictionary=dictionary)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# Visualization\n",
    "Here we visualize with pyLDAvis in an interactive mode the LDA analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "\n * Not all rows (distributions) in topic_term_dists sum to 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-7eb17ddcc35d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlda_display\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mldamodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_display\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyLDAvis/gensim.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \"\"\"\n\u001b[1;32m    118\u001b[0m     \u001b[0mopts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_topic_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mvis_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, R, lambda_step, mds, n_jobs, plot_opts, sort_topics)\u001b[0m\n\u001b[1;32m    372\u001b[0m    \u001b[0mdoc_lengths\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0m_series_with_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'doc_length'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m    \u001b[0mvocab\u001b[0m            \u001b[0;34m=\u001b[0m \u001b[0m_series_with_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vocab'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m    \u001b[0m_input_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_term_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_topic_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_frequency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m    \u001b[0mR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36m_input_validate\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m     63\u001b[0m    \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_input_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValidationError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m' * '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: \n * Not all rows (distributions) in topic_term_dists sum to 1."
     ]
    }
   ],
   "source": [
    "import pyLDAvis.gensim\n",
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(wl)\n",
    "corpus = [dictionary.doc2bow(text) for text in wl]\n",
    "lda_display = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocab Splitting\n",
    "\n",
    "Here we get the vocab for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Representatives: 0\n",
      "Representatives: 1000000\n",
      "Representatives: 2000000\n",
      "Representatives: 3000000\n",
      "Representatives: 4000000\n",
      "Finished representatives\n",
      "Overlap Matrix of Vocab\n",
      "          0         1         2         3         4         5         6  \\\n",
      "0  1.000000  0.433109  0.296577  0.377850  0.430533  0.419403  0.396045   \n",
      "1  0.433109  1.000000  0.257183  0.369511  0.434925  0.433718  0.370654   \n",
      "2  0.296577  0.257183  1.000000  0.275627  0.249150  0.231479  0.366931   \n",
      "3  0.377850  0.369511  0.275627  1.000000  0.363572  0.352985  0.356752   \n",
      "4  0.430533  0.434925  0.249150  0.363572  1.000000  0.429736  0.359918   \n",
      "5  0.419403  0.433718  0.231479  0.352985  0.429736  1.000000  0.345667   \n",
      "6  0.396045  0.370654  0.366931  0.356752  0.359918  0.345667  1.000000   \n",
      "7  0.441388  0.441597  0.276756  0.374693  0.437972  0.428611  0.387070   \n",
      "8  0.409641  0.430209  0.212272  0.340930  0.430837  0.432849  0.325636   \n",
      "9  0.382166  0.373526  0.296171  0.338647  0.364056  0.352732  0.364138   \n",
      "\n",
      "          7         8         9  \n",
      "0  0.441388  0.409641  0.382166  \n",
      "1  0.441597  0.430209  0.373526  \n",
      "2  0.276756  0.212272  0.296171  \n",
      "3  0.374693  0.340930  0.338647  \n",
      "4  0.437972  0.430837  0.364056  \n",
      "5  0.428611  0.432849  0.352732  \n",
      "6  0.387070  0.325636  0.364138  \n",
      "7  1.000000  0.422656  0.375391  \n",
      "8  0.422656  1.000000  0.341142  \n",
      "9  0.375391  0.341142  1.000000  \n",
      "\n",
      "\n",
      "Average for each topic\n",
      "          0\n",
      "0  0.398524\n",
      "1  0.393826\n",
      "2  0.273572\n",
      "3  0.350063\n",
      "4  0.388966\n",
      "5  0.380798\n",
      "6  0.363646\n",
      "7  0.398459\n",
      "8  0.371797\n",
      "9  0.354219\n",
      "\n",
      "\n",
      "Average overall\n",
      "0.36738694935167243\n",
      "\n",
      "\n",
      "Max\n",
      "0.4415966844803141\n"
     ]
    }
   ],
   "source": [
    "def get_representatives_lda(word_list, lda_model, dictionary, num_topics=10):\n",
    "    max_line = [[]] * num_topics\n",
    "    max_class = [0] * num_topics \n",
    "    class_vocab = {i:{} for i in range(0, num_topics)}\n",
    "    \n",
    "    for line, idx in zip(word_list, range(len(word_list))):\n",
    "        #print(line)\n",
    "        new_doc_bow = dictionary.doc2bow(line)\n",
    "        s = ldamodel.get_document_topics(new_doc_bow)\n",
    "        s.sort(key=lambda x: x[1])\n",
    "        t_max_class, t_max_perc = s[-1]\n",
    "        \n",
    "        for word in line:\n",
    "            if word in class_vocab[t_max_class]:\n",
    "                class_vocab[t_max_class][word] += 1\n",
    "            else:\n",
    "                class_vocab[t_max_class][word] = 1\n",
    "        \n",
    "        if max_class[t_max_class] < t_max_perc:\n",
    "            max_class[t_max_class] = t_max_perc\n",
    "            max_line[t_max_class] = line\n",
    "        \n",
    "        if idx % 1000000 == 0:\n",
    "            print(\"Representatives: \" + str(idx))\n",
    "        \n",
    "    print(\"Finished representatives\")\n",
    "    return max_line, max_class, class_vocab\n",
    "\n",
    "# max_l, max_c, class_vocab = get_representatives_lda(wl, ldamodel, dictionary)\n",
    "\"\"\"\n",
    "for idx in range(len(max_l)):\n",
    "    print(\"\\n\")\n",
    "    print(\"\\n\")\n",
    "    print(max_c[idx])\n",
    "    print(max_l[idx])\n",
    "\"\"\"  \n",
    "\n",
    "# get overlap\n",
    "num_topics = 10\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_overlap_matrix(class_vocab, num_topics):\n",
    "    over_lap_matrix = np.ones((num_topics, num_topics))\n",
    "\n",
    "    for c in range(0, num_topics):\n",
    "        for c2 in range(0, num_topics):\n",
    "            # Skip self\n",
    "            if c2 == c:\n",
    "                continue\n",
    "            # overlap\n",
    "            words_in_c2_and_c1 = 0\n",
    "            total_words_c2 = 0\n",
    "            for word in class_vocab[c2]:\n",
    "                total_words_c2 += 1  # class_vocab[c2][word]\n",
    "                if word in class_vocab[c]:\n",
    "                    words_in_c2_and_c1 += 1\n",
    "            c_factor = len(class_vocab[c])/(len(class_vocab[c2]) + len(class_vocab[c]))\n",
    "            c2_factor = len(class_vocab[c2])/(len(class_vocab[c2]) + len(class_vocab[c]))\n",
    "            over_lap_matrix[c, c2] = (words_in_c2_and_c1/len(class_vocab[c2])) * c2_factor + (words_in_c2_and_c1/len(class_vocab[c])) * c_factor\n",
    "    \n",
    "    return over_lap_matrix\n",
    "\n",
    "\n",
    "over_lap_matrix = get_overlap_matrix(get_representatives_lda(wl_full, ldamodel, dictionary, num_topics=num_topics)[2], num_topics)\n",
    "\n",
    "print(\"Overlap Matrix of Vocab\")\n",
    "print(pd.DataFrame(over_lap_matrix))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Average for each topic\")\n",
    "# Remove diagonal as it skews data\n",
    "print(pd.DataFrame(np.average(over_lap_matrix[~np.eye(over_lap_matrix.shape[0],dtype=bool)].reshape(over_lap_matrix.shape[0],-1), axis=1)))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Average overall\")\n",
    "print(np.average(over_lap_matrix[~np.eye(over_lap_matrix.shape[0],dtype=bool)].reshape(over_lap_matrix.shape[0],-1)))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Max\")\n",
    "print(np.max(over_lap_matrix[~np.eye(over_lap_matrix.shape[0],dtype=bool)].reshape(over_lap_matrix.shape[0],-1)))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
