{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Topic and Word Relationships\n",
    "\n",
    "Here we explore different ways that the the newstest 2015 dataset can be looked at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file\n",
      "Finished loading in file\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def read_file(file_path, max_amount=math.inf):\n",
    "    \"\"\"\n",
    "    Reads a number of lines from file and return list of lines\n",
    "    :param file_path:\n",
    "    :param max_amount:\n",
    "    :return: list[str]\n",
    "    \"\"\"\n",
    "    line_list = []\n",
    "\n",
    "    with open(file=file_path) as f:\n",
    "        curr_idx = 0\n",
    "        while curr_idx < max_amount:\n",
    "            line_list.append(f.readline())\n",
    "            curr_idx += 1\n",
    "\n",
    "    return line_list\n",
    "\n",
    "print(\"Loading file\")\n",
    "l_raw = read_file('./trg.shuf', 5000000)\n",
    "print(\"Finished loading in file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/nikita/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/nikita/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size train: 47708\n",
      "Vocab size full: 1001125\n",
      "Finished Dictionary\n",
      "Finished corpus\n",
      "Topics: \n",
      "(0, '0.004*\"record\" + 0.003*\"hear\" + 0.002*\"map\" + 0.002*\"transfer\"')\n",
      "(1, '0.012*\"european\" + 0.008*\"day\" + 0.007*\"right\" + 0.006*\"market\"')\n",
      "(2, '0.005*\"phone\" + 0.004*\"article\" + 0.004*\"legal\" + 0.003*\"monday\"')\n",
      "(3, '0.018*\"night\" + 0.014*\"2016\" + 0.011*\"sat\" + 0.006*\"currently\"')\n",
      "(4, '0.004*\"far\" + 0.004*\"church\" + 0.002*\"role\" + 0.002*\"figure\"')\n",
      "(5, '0.009*\"page\" + 0.004*\"generate\" + 0.003*\"new\" + 0.003*\"second\"')\n",
      "(6, '0.012*\"post\" + 0.006*\"2010\" + 0.006*\"2007\" + 0.005*\"2006\"')\n",
      "(7, '0.004*\"relate\" + 0.003*\"press\" + 0.002*\"element\" + 0.002*\"trip\"')\n",
      "(8, '0.004*\"german\" + 0.004*\"language\" + 0.003*\"love\" + 0.003*\"french\"')\n",
      "(9, '0.008*\"council\" + 0.006*\"2012\" + 0.006*\"price\" + 0.005*\"therefore\"')\n",
      "(10, '0.008*\"also\" + 0.008*\"one\" + 0.008*\"say\" + 0.007*\"time\"')\n",
      "(11, '0.015*\"http\" + 0.004*\"image\" + 0.003*\"tel\" + 0.003*\"economy\"')\n",
      "(12, '0.007*\"hotel\" + 0.006*\"visit\" + 0.006*\"2013\" + 0.005*\"2014\"')\n",
      "(13, '0.009*\"europe\" + 0.008*\"room\" + 0.005*\"free\" + 0.005*\"law\"')\n",
      "(14, '0.003*\"film\" + 0.003*\"june\" + 0.003*\"various\" + 0.003*\"training\"')\n",
      "(15, '0.004*\"let\" + 0.003*\"transport\" + 0.002*\"search\" + 0.002*\"nation\"')\n",
      "(16, '0.002*\"difference\" + 0.002*\"spirit\" + 0.002*\"blue\" + 0.002*\"voice\"')\n",
      "(17, '0.005*\"august\" + 0.005*\"...\" + 0.004*\"saturday\" + 0.004*\"apartment\"')\n",
      "(18, '0.005*\"2011\" + 0.004*\"update\" + 0.003*\"feel\" + 0.003*\"happen\"')\n",
      "(19, '0.006*\"2015\" + 0.006*\"woman\" + 0.005*\"great\" + 0.003*\"meet\"')\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import operator\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "\n",
    "\n",
    "def process_word(word, remove_stop=True, lemmatize_and_morph=True):\n",
    "    \"\"\"\n",
    "    Returns process word\n",
    "    :param word:\n",
    "    :return: str word\n",
    "    \"\"\"\n",
    "    # lower\n",
    "    word = word.lower()\n",
    "    \n",
    "    # Remove symbols\n",
    "    if len(word) < 3:\n",
    "        return None\n",
    "    \n",
    "    # Get morph\n",
    "    if lemmatize_and_morph is True:\n",
    "        t_word = wn.morphy(word)\n",
    "        if t_word is not None:\n",
    "            word = t_word\n",
    "\n",
    "        # Get lemma\n",
    "        word = WordNetLemmatizer().lemmatize(word)\n",
    "    \n",
    "    # Remove stop\n",
    "    if remove_stop is True and word in en_stop:\n",
    "        return None\n",
    "    return word\n",
    "\n",
    "\n",
    "def preprocess_all_words(word_list):\n",
    "    ret = []\n",
    "    for line in word_list:\n",
    "        temp = []\n",
    "        for word in line:\n",
    "            w = process_word(word, remove_stop=True, lemmatize_and_morph=False)\n",
    "            if w is not None:\n",
    "                temp.append(w)\n",
    "        ret.append(temp)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def process_bpe(line_list, seq=\"@@\", rm_words=[\"\\n\"], lemmatize_and_morph=True):\n",
    "    \"\"\"\n",
    "    Transforms line list form read_file to list[list[str=words]], and bpe words merged together\n",
    "    :param line_list:\n",
    "    :param seq:\n",
    "    :param rm_words:\n",
    "    :return: list[list[str=words]]\n",
    "    \"\"\"\n",
    "    full_word_list = []\n",
    "    for line in line_list:\n",
    "        temp_words = line.split()\n",
    "        full_words = []\n",
    "        idx = 0\n",
    "        while idx < len(temp_words):\n",
    "            if temp_words[idx].endswith(seq):\n",
    "                temp_str = \"\"\n",
    "                while temp_words[idx].endswith(seq):\n",
    "                    temp_str += temp_words[idx][:-(len(seq))]\n",
    "                    idx += 1\n",
    "                temp_str += temp_words[idx]\n",
    "                if temp_str not in rm_words:\n",
    "                    w = process_word(temp_str, lemmatize_and_morph=lemmatize_and_morph)\n",
    "                    if w is not None:\n",
    "                        full_words.append(w)\n",
    "                idx += 1\n",
    "            else:\n",
    "                if temp_words[idx] not in rm_words:\n",
    "                    w = process_word(temp_words[idx], lemmatize_and_morph=lemmatize_and_morph)\n",
    "                    if w is not None:\n",
    "                        full_words.append(w)\n",
    "                idx += 1\n",
    "        full_word_list.append(full_words)\n",
    "    return full_word_list\n",
    "\n",
    "\n",
    "def get_word_count(word_list):\n",
    "    \"\"\"\n",
    "    Returns word count.\n",
    "    :param word_list:\n",
    "    :return: List[(str=word, int=count)]\n",
    "    \"\"\"\n",
    "    word_count = {}\n",
    "    for line in word_list:\n",
    "        for word in line:\n",
    "            if word in word_count:\n",
    "                word_count[word] += 1\n",
    "            else:\n",
    "                word_count[word] = 1\n",
    "\n",
    "    return sorted(word_count.items(), key=operator.itemgetter(1))\n",
    "\n",
    "\n",
    "def get_topics_lda(word_list, dictionary, num_topics=10):\n",
    "    corpus = [dictionary.doc2bow(text) for text in word_list]\n",
    "    print(\"Finished corpus\")\n",
    "    \n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)\n",
    "    topics = ldamodel.print_topics(num_words=4)\n",
    "    print(\"Topics: \")\n",
    "    for topic in topics:\n",
    "        print(topic)\n",
    "\n",
    "    return topics, ldamodel, corpus, dictionary\n",
    "\n",
    "num_topics = 20\n",
    "\n",
    "import random\n",
    "l = random.sample(l_raw, 50000)\n",
    "\n",
    "wl = process_bpe(line_list=l)\n",
    "print(\"Vocab size train: \" + str(len(get_word_count(word_list=wl))))\n",
    "\n",
    "wl_full = process_bpe(line_list=l_raw, lemmatize_and_morph=False)\n",
    "print(\"Vocab size full: \" + str(len(get_word_count(word_list=wl_full))))\n",
    "\n",
    "dictionary = corpora.Dictionary(wl_full)\n",
    "print(\"Finished Dictionary\")\n",
    "\n",
    "t, ldamodel,_, dictionary = get_topics_lda(word_list=preprocess_all_words(wl), dictionary=dictionary, num_topics=num_topics)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# Visualization\n",
    "Here we visualize with pyLDAvis in an interactive mode the LDA analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "\n * Not all rows (distributions) in topic_term_dists sum to 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-7eb17ddcc35d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlda_display\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mldamodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_display\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyLDAvis/gensim.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \"\"\"\n\u001b[1;32m    118\u001b[0m     \u001b[0mopts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_topic_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mvis_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, R, lambda_step, mds, n_jobs, plot_opts, sort_topics)\u001b[0m\n\u001b[1;32m    372\u001b[0m    \u001b[0mdoc_lengths\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0m_series_with_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'doc_length'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m    \u001b[0mvocab\u001b[0m            \u001b[0;34m=\u001b[0m \u001b[0m_series_with_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vocab'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m    \u001b[0m_input_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_term_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_topic_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_frequency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m    \u001b[0mR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36m_input_validate\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m     63\u001b[0m    \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_input_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValidationError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m' * '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: \n * Not all rows (distributions) in topic_term_dists sum to 1."
     ]
    }
   ],
   "source": [
    "import pyLDAvis.gensim\n",
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(wl)\n",
    "corpus = [dictionary.doc2bow(text) for text in wl]\n",
    "lda_display = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocab Splitting\n",
    "\n",
    "Here we get the vocab for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Representatives: 0\n",
      "Representatives: 100000\n",
      "Representatives: 200000\n",
      "Representatives: 300000\n",
      "Representatives: 400000\n",
      "Representatives: 500000\n",
      "Representatives: 600000\n",
      "Representatives: 700000\n",
      "Representatives: 800000\n",
      "Representatives: 900000\n",
      "Representatives: 1000000\n",
      "Representatives: 1100000\n",
      "Representatives: 1200000\n",
      "Representatives: 1300000\n",
      "Representatives: 1400000\n",
      "Representatives: 1500000\n",
      "Representatives: 1600000\n",
      "Representatives: 1700000\n",
      "Representatives: 1800000\n",
      "Representatives: 1900000\n",
      "Representatives: 2000000\n",
      "Representatives: 2100000\n",
      "Representatives: 2200000\n",
      "Representatives: 2300000\n",
      "Representatives: 2400000\n",
      "Representatives: 2500000\n",
      "Representatives: 2600000\n",
      "Representatives: 2700000\n",
      "Representatives: 2800000\n",
      "Representatives: 2900000\n",
      "Representatives: 3000000\n",
      "Representatives: 3100000\n",
      "Representatives: 3200000\n",
      "Representatives: 3300000\n",
      "Representatives: 3400000\n",
      "Representatives: 3500000\n",
      "Representatives: 3600000\n",
      "Representatives: 3700000\n",
      "Representatives: 3800000\n",
      "Representatives: 3900000\n",
      "Representatives: 4000000\n",
      "Representatives: 4100000\n",
      "Representatives: 4200000\n",
      "Representatives: 4300000\n",
      "Representatives: 4400000\n",
      "Representatives: 4500000\n",
      "Representatives: 4600000\n",
      "Representatives: 4700000\n",
      "Representatives: 4800000\n",
      "Representatives: 4900000\n",
      "Finished representatives\n"
     ]
    }
   ],
   "source": [
    "def get_representatives_lda(word_list, lda_model, dictionary, num_topics=10):\n",
    "    max_line = [[]] * num_topics\n",
    "    max_class = [0] * num_topics \n",
    "    class_vocab = {i:{} for i in range(0, num_topics)}\n",
    "    \n",
    "    for line, idx in zip(word_list, range(len(word_list))):\n",
    "        #print(line)\n",
    "        new_doc_bow = dictionary.doc2bow(line)\n",
    "        s = ldamodel.get_document_topics(new_doc_bow)\n",
    "        s.sort(key=lambda x: x[1])\n",
    "        t_max_class, t_max_perc = s[-1]\n",
    "        \n",
    "        for word in line:\n",
    "            if word in class_vocab[t_max_class]:\n",
    "                class_vocab[t_max_class][word] += 1\n",
    "            else:\n",
    "                class_vocab[t_max_class][word] = 1\n",
    "        \n",
    "        if max_class[t_max_class] < t_max_perc:\n",
    "            max_class[t_max_class] = t_max_perc\n",
    "            max_line[t_max_class] = line\n",
    "        \n",
    "        if idx % 100000 == 0:\n",
    "            print(\"Representatives: \" + str(idx))\n",
    "        \n",
    "    print(\"Finished representatives\")\n",
    "    return max_line, max_class, class_vocab\n",
    "\n",
    "\n",
    "_, _, class_vocab = get_representatives_lda(wl_full, ldamodel, dictionary, num_topics=num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap Matrix of Vocab\n",
      "          0         1         2         3         4         5         6   \\\n",
      "0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "1   0.320151  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "2   0.361944  0.298270  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "3   0.365961  0.287118  0.353892  0.000000  0.000000  0.000000  0.000000   \n",
      "4   0.377716  0.341758  0.346009  0.354175  0.000000  0.000000  0.000000   \n",
      "5   0.349275  0.284684  0.322226  0.320229  0.334651  0.000000  0.000000   \n",
      "6   0.388647  0.331071  0.357600  0.374928  0.383218  0.340301  0.000000   \n",
      "7   0.344817  0.300846  0.318238  0.314460  0.340512  0.332236  0.334143   \n",
      "8   0.399386  0.328405  0.364263  0.370648  0.384095  0.347254  0.394343   \n",
      "9   0.379233  0.382891  0.354255  0.364495  0.381506  0.329973  0.403175   \n",
      "10  0.184421  0.366902  0.174013  0.161903  0.206407  0.172627  0.189058   \n",
      "11  0.241602  0.230506  0.234845  0.234814  0.239068  0.221540  0.240211   \n",
      "12  0.343905  0.372391  0.320273  0.332063  0.361658  0.307552  0.349756   \n",
      "13  0.360842  0.415157  0.337360  0.337722  0.377623  0.317173  0.368481   \n",
      "14  0.380584  0.321190  0.350885  0.361667  0.367551  0.330166  0.383613   \n",
      "15  0.394971  0.315757  0.358011  0.360618  0.380426  0.353956  0.383091   \n",
      "16  0.354307  0.296894  0.321669  0.321285  0.336649  0.330233  0.340625   \n",
      "17  0.344316  0.282081  0.329622  0.336388  0.339969  0.307385  0.341496   \n",
      "18  0.393442  0.316877  0.358336  0.365037  0.381362  0.350407  0.388806   \n",
      "19  0.392783  0.397403  0.361166  0.368447  0.397206  0.339218  0.402822   \n",
      "\n",
      "          7         8         9         10        11        12        13  \\\n",
      "0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "1   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "2   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "3   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "4   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "5   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "6   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "7   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "8   0.342563  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "9   0.334070  0.386306  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "10  0.191289  0.188337  0.235512  0.000000  0.000000  0.000000  0.000000   \n",
      "11  0.225191  0.241795  0.248684  0.152626  0.000000  0.000000  0.000000   \n",
      "12  0.318659  0.346002  0.372704  0.266714  0.242998  0.000000  0.000000   \n",
      "13  0.331015  0.367089  0.402120  0.286887  0.246135  0.405674  0.000000   \n",
      "14  0.330078  0.380523  0.380373  0.186397  0.234877  0.337710  0.360630   \n",
      "15  0.346261  0.393858  0.373191  0.184843  0.237371  0.342170  0.357321   \n",
      "16  0.352672  0.354439  0.334115  0.182177  0.226409  0.312672  0.326285   \n",
      "17  0.307803  0.345290  0.338856  0.169489  0.227621  0.317920  0.323064   \n",
      "18  0.345954  0.392705  0.379629  0.183973  0.241566  0.340190  0.356703   \n",
      "19  0.346722  0.402245  0.421998  0.249355  0.252753  0.393008  0.425718   \n",
      "\n",
      "          14        15        16        17        18   19  \n",
      "0   0.000000  0.000000  0.000000  0.000000  0.000000  0.0  \n",
      "1   0.000000  0.000000  0.000000  0.000000  0.000000  0.0  \n",
      "2   0.000000  0.000000  0.000000  0.000000  0.000000  0.0  \n",
      "3   0.000000  0.000000  0.000000  0.000000  0.000000  0.0  \n",
      "4   0.000000  0.000000  0.000000  0.000000  0.000000  0.0  \n",
      "5   0.000000  0.000000  0.000000  0.000000  0.000000  0.0  \n",
      "6   0.000000  0.000000  0.000000  0.000000  0.000000  0.0  \n",
      "7   0.000000  0.000000  0.000000  0.000000  0.000000  0.0  \n",
      "8   0.000000  0.000000  0.000000  0.000000  0.000000  0.0  \n",
      "9   0.000000  0.000000  0.000000  0.000000  0.000000  0.0  \n",
      "10  0.000000  0.000000  0.000000  0.000000  0.000000  0.0  \n",
      "11  0.000000  0.000000  0.000000  0.000000  0.000000  0.0  \n",
      "12  0.000000  0.000000  0.000000  0.000000  0.000000  0.0  \n",
      "13  0.000000  0.000000  0.000000  0.000000  0.000000  0.0  \n",
      "14  0.000000  0.000000  0.000000  0.000000  0.000000  0.0  \n",
      "15  0.372744  0.000000  0.000000  0.000000  0.000000  0.0  \n",
      "16  0.334120  0.358547  0.000000  0.000000  0.000000  0.0  \n",
      "17  0.334164  0.351195  0.310443  0.000000  0.000000  0.0  \n",
      "18  0.375801  0.397032  0.355486  0.351076  0.000000  0.0  \n",
      "19  0.387556  0.384396  0.352386  0.340098  0.389442  0.0  \n",
      "\n",
      "\n",
      "Average overall\n",
      "0.32802828229604136\n",
      "\n",
      "\n",
      "Max\n",
      "0.4257181223178988\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# max_l, max_c, class_vocab = get_representatives_lda(wl, ldamodel, dictionary)\n",
    "\"\"\"\n",
    "for idx in range(len(max_l)):\n",
    "    print(\"\\n\")\n",
    "    print(\"\\n\")\n",
    "    print(max_c[idx])\n",
    "    print(max_l[idx])\n",
    "\"\"\"  \n",
    "\n",
    "# get overlap\n",
    "#num_topics = 20\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_overlap_matrix(class_vocab, num_topics):\n",
    "    over_lap_matrix = np.zeros((num_topics, num_topics))\n",
    "    over_lap_vocab = {}\n",
    "    \n",
    "    for c in range(0, num_topics):\n",
    "        for c2 in range(0, num_topics):\n",
    "            # Skip self\n",
    "            if c2 >= c:\n",
    "                continue\n",
    "            # overlap\n",
    "            words_in_c2_and_c1 = 0\n",
    "            total_words_c2 = 0\n",
    "            \n",
    "            for word in class_vocab[c2]:\n",
    "                if word not in over_lap_vocab:\n",
    "                    over_lap_vocab[word] = np.zeros((num_topics, num_topics))\n",
    "                \n",
    "                total_words_c2 += 1 \n",
    "                if word in class_vocab[c]:\n",
    "                    words_in_c2_and_c1 += 1\n",
    "                    over_lap_vocab[word][c, c2] = 1\n",
    "            # Normalize        \n",
    "            c_factor = len(class_vocab[c])/(len(class_vocab[c2]) + len(class_vocab[c]))\n",
    "            c2_factor = len(class_vocab[c2])/(len(class_vocab[c2]) + len(class_vocab[c]))\n",
    "            over_lap_matrix[c, c2] = (words_in_c2_and_c1/len(class_vocab[c2])) * c2_factor + (words_in_c2_and_c1/len(class_vocab[c])) * c_factor\n",
    "    \n",
    "    return over_lap_matrix, over_lap_vocab\n",
    "\n",
    "\n",
    "over_lap_matrix, over_lap_vocab = get_overlap_matrix(class_vocab, num_topics)\n",
    "\n",
    "over_lap_sum = 0\n",
    "over_lap_count = 0\n",
    "for c in range(0, num_topics):\n",
    "    for c2 in range(0, num_topics):\n",
    "        # Skip self\n",
    "        if c2 >= c:\n",
    "            continue\n",
    "        over_lap_sum += over_lap_matrix[c, c2]\n",
    "        over_lap_count += 1\n",
    "        \n",
    "over_lap_avg = over_lap_sum/over_lap_count\n",
    "\n",
    "print(\"Overlap Matrix of Vocab\")\n",
    "print(pd.DataFrame(over_lap_matrix))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Average overall\")\n",
    "print(over_lap_avg)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Max\")\n",
    "print(np.max(over_lap_matrix))\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "\n",
    "\n",
    "## LDA\n",
    "\n",
    "### 20 Topics, 50000 clustering, 5 million testing:\n",
    "\n",
    "Without stop words, overall average 32.8% (max 42.5%).\n",
    "\n",
    "\n",
    "### TODO: 5, 10 Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring vocab overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 overlapping words: \n",
      "[('memo', 190.0), ('danny', 190.0), ('slots', 190.0), ('cottages', 190.0), ('lübeck', 190.0), ('firefox', 190.0), ('ready-made', 190.0), ('modest', 190.0), ('martha', 190.0), ('relate', 190.0), ('rubble', 190.0), ('olives', 190.0), ('whatsoever', 190.0), ('132', 190.0), ('salmonella', 190.0), ('giorgio', 190.0), ('camping', 190.0), ('2.7', 190.0), ('gallons', 190.0), ('leaked', 190.0), ('underwater', 190.0), ('wich', 190.0), ('5-10', 190.0), ('151', 190.0), ('169', 190.0), ('ecu', 190.0), ('dancer', 190.0), ('wholesale', 190.0), ('screens', 190.0), ('185', 190.0), ('hierarchy', 190.0), ('offline', 190.0), ('distortions', 190.0), ('revolt', 190.0), ('9.5', 190.0), ('puzzles', 190.0), ('geelong', 190.0), ('genuinely', 190.0), ('toyota', 190.0), ('ornamental', 190.0), ('aig', 190.0), ('harvest', 190.0), ('adjoining', 190.0), ('naval', 190.0), ('clusters', 190.0), ('hamburger', 190.0), ('147', 190.0), ('mafia', 190.0), ('n.j.', 190.0), ('christie', 190.0), ('hobbies', 190.0), ('990', 190.0), ('cohesion', 190.0), ('anita', 190.0), ('disagree', 190.0), ('kms', 190.0), ('maple', 190.0), ('index.php', 190.0), ('13.2', 190.0), ('pillow', 190.0), ('listing', 190.0), ('garments', 190.0), ('equipments', 190.0), ('olympus', 190.0), ('booklet', 190.0), ('metric', 190.0), ('proxy', 190.0), ('disruption', 190.0), ('crushing', 190.0), ('fermented', 190.0), ('subscribe', 190.0), ('versus', 190.0), ('stationary', 190.0), ('corp', 190.0), ('1700', 190.0), ('horror', 190.0), ('prescription', 190.0), ('flaws', 190.0), ('laundry', 190.0), ('kyle', 190.0), ('unpleasant', 190.0), ('demos', 190.0), ('cowboys', 190.0), ('walsh', 190.0), ('hurdles', 190.0), ('341', 190.0), ('bahn', 190.0), ('embraced', 190.0), ('manipulated', 190.0), ('suitcase', 190.0), ('branded', 190.0), ('northampton', 190.0), ('internally', 190.0), ('attachments', 190.0), ('285', 190.0), ('intercepted', 190.0), ('protective', 190.0), ('daniela', 190.0), ('landslides', 190.0)]\n"
     ]
    }
   ],
   "source": [
    "vocab_overlap_list = [(word, np.sum(over_lap_vocab[word])) for word in over_lap_vocab]\n",
    "sort_vocab_overlap = sorted(vocab_overlap_list, key=lambda tup: tup[1])\n",
    "\n",
    "print(\"Top 100 overlapping words: \")\n",
    "print(sort_vocab_overlap[-100:-1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([(0.0, 692748), (1.0, 113524), (3.0, 49371), (6.0, 28685), (10.0, 19303), (15.0, 13753), (21.0, 10263), (28.0, 7957), (36.0, 6328), (45.0, 5313), (55.0, 4354), (66.0, 3923), (78.0, 3312), (91.0, 2996), (105.0, 2718), (120.0, 2545), (136.0, 2508), (153.0, 2538), (171.0, 2819), (190.0, 9841)])\n",
      "984799\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "over_lap_distribution = {}\n",
    "for word, count in sort_vocab_overlap:\n",
    "    if count in over_lap_distribution:\n",
    "        over_lap_distribution[count] += 1\n",
    "    else:\n",
    "        over_lap_distribution[count] = 1\n",
    "\n",
    "over_lap_distribution = over_lap_distribution.items()\n",
    "print(over_lap_distribution)\n",
    "print(sum([v[1] for v in over_lap_distribution]))\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(*zip(*over_lap_distribution))\n",
    "plt.title(\"How often each intersections take place\")\n",
    "plt.xlabel(\"Occurrences of intersection of word between two topics\")\n",
    "plt.ylabel(\"Amount of words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(over_lap_vocab['makarov'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
